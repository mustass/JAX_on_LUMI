# -*- coding: utf-8 -*-
"""sharding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XJxxE7-49oa6eP-0Q_-dPCnOkU9vDAQv

# Scale up Flax Modules on multiple devices

This guide shows how to scale up [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html) on multiple devices and hosts using [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) (formerly [`experimental.pjit`](https://jax.readthedocs.io/en/latest/jax.experimental.pjit.html#module-jax.experimental.pjit)) and [`flax.linen`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/index.html).

## Flax and `jax.jit` scaled up

[`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) follows the [Single Program Multi Data (SPMD)](https://jax.readthedocs.io/en/latest/glossary.html#term-SPMD) paradigm and automatically compiles your code to run it on multiple devices. You need to only specify how you want the input and output of your code to be partitioned, and the compiler will figure out how to: 1) partition everything inside; and 2) compile inter-device communications.

Flax provides several functionalities that can help you use auto-SPMD on [Flax Modules](https://flax.readthedocs.io/en/latest/developer_notes/module_lifecycle.html), including:

1. An interface to specify partitions of your data when defining [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html).
2. Utility functions to generate the sharding information that `jax.jit` requires to run.
3. An interface to customize your axis names called "logical axis annotations" to decouple both your Module code and partition plan to experiment with different partition layouts more easily.

You can learn more about `jax.jit` APIs for scaling up in [JAX in multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html) and [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) on JAX's documentation site.

## Setup

Import some necessary dependencies.

**Note:** This guide uses the `--xla_force_host_platform_device_count=8` flag to emulate multiple devices in a CPU environment in a Google Colab/Jupyter Notebook. You don't need this if you are already using a multi-device TPU environment.
"""

# Once Flax v0.6.10 is released, there is no need to do this.
# ! pip3 install -qq "git+https://github.com/google/flax.git@main#egg=flax"

import os
#os.environ["XLA_FLAGS"] = '--xla_force_host_platform_device_count=8'

import functools
from typing import Optional, Callable

import numpy as np
import jax
from jax import lax, random, numpy as jnp

import flax
from flax import struct, traverse_util, linen as nn
from flax.core import freeze, unfreeze
from flax.training import train_state, checkpoints

import optax # Optax for common losses and optimizers.

print(f'We have 8 fake JAX devices now: {jax.devices()}')

"""The code below shows how to import and set up the JAX-level device API, following JAX's [Distributed arrays and automatic parallelization](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) guide:

1. Start a 2x4 device `mesh` (8 devices) using JAX's `mesh_utils.create_device_mesh`. This layout is the same as the one of a [TPU v3-8](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#single_tpu_board).

2. Annotate each axis with a name using the `axis_names` parameter in `jax.sharding.Mesh`. A typical way to annotate axis names is `axis_name=('data', 'model')`, where:
  * `'data'`: the mesh dimension used for data-parallel sharding of the batch dimension of inputs and activations.
  * `'model'`: the mesh dimension used for sharding parameters of the model across devices.

3. Make a simple utility function `mesh_sharding` for generating a sharding object from the mesh and any layout.
"""

from jax.sharding import Mesh, PartitionSpec, NamedSharding
from jax.lax import with_sharding_constraint
from jax.experimental import mesh_utils

import torch
import torchvision
import torchvision.transforms as transforms
def get_dataloader(batch_size=64):
    transform = transforms.Compose([transforms.ToTensor()])
    trainset = torchvision.datasets.CIFAR10(root='./scratch/data', train=True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)
    return trainloader

loader = get_dataloader()

# Create a mesh and annotate each axis with a name.
device_mesh = mesh_utils.create_device_mesh((8,))
print(device_mesh)

mesh = Mesh(devices=device_mesh, axis_names=('data'))
print(mesh)

def mesh_sharding(pspec: PartitionSpec) -> NamedSharding:
  return NamedSharding(mesh, pspec)

"""## Define a layer

Before defining a simple model, create an example layer called `DotReluDot` (by subclassing `flax.linen.Module`). The layer creates two parameters `W1` and `W2` for dot product multiplication, and uses the `jax.nn.relu` (ReLU) activation function in-between.

To shard the parameters efficiently, apply the following APIs to annotate the parameters and intermediate variables:

1. Use [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) to decorate the initializer function when creating sub-layers or raw parameters.

2. Apply [`jax.lax.with_sharding_constraint`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.with_sharding_constraint.html) (formerly, `pjit.with_sharding_constraint`) to annotate intermediate variables like `y` and `z` to force a particular sharding pattern when the ideal constraint is known.

  * This step is optional, but can sometimes help auto-SPMD to partition efficiently. In the example below, the call is not required, because XLA will figure out the same sharding layout for `y` and `z` regardless.
"""

class CNN_sharded(nn.Module):
    num_classes: int
    dense_init: Callable = nn.initializers.xavier_normal()
    @nn.compact
    def __call__(self, x):
        x = nn.relu(nn.Conv(32, (3, 3), padding='SAME', kernel_init=nn.with_partitioning(self.dense_init, (None)))(x))
        x = with_sharding_constraint(x,mesh_sharding(PartitionSpec(None)))
        x = nn.max_pool(x, (2, 2))
        x = with_sharding_constraint(x,mesh_sharding(PartitionSpec(None)))
        x = nn.relu(nn.Conv(64, (3, 3), padding='SAME', kernel_init=nn.with_partitioning(self.dense_init, ( None)))(x))
        x = with_sharding_constraint(x,mesh_sharding(PartitionSpec(None)))
        x = nn.max_pool(x, (2, 2))
        x = with_sharding_constraint(x,mesh_sharding(PartitionSpec(None)))
        x = x.reshape((x.shape[0], -1))  # Flatten
        x = nn.relu(nn.Dense(128,kernel_init=nn.with_partitioning(self.dense_init, (None)),
                            use_bias=False
                 )(x))
        x = with_sharding_constraint(x,mesh_sharding(PartitionSpec(None)))
        x = nn.Dense(self.num_classes,kernel_init=nn.with_partitioning(self.dense_init, (None)),
                            use_bias=False,
                 )(x)
        x = with_sharding_constraint(x,mesh_sharding(PartitionSpec(None)))

        return x

class CNN(nn.Module):
    num_classes: int
    dense_init: Callable = nn.initializers.xavier_normal()
    @nn.compact
    def __call__(self, x):
        x = nn.relu(nn.Conv(32, (3, 3), padding='SAME', kernel_init=self.dense_init)(x))
        x = nn.max_pool(x, (2, 2))
        x = nn.relu(nn.Conv(64, (3, 3), padding='SAME', kernel_init=self.dense_init)(x))
        x = nn.max_pool(x, (2, 2))
        x = x.reshape((x.shape[0], -1))  # Flatten
        x = nn.relu(nn.Dense(128,kernel_init=self.dense_init,
                            use_bias=False
                 )(x))
        x = nn.Dense(self.num_classes,kernel_init=self.dense_init,
                            use_bias=False,
                 )(x)
        return x

"""Note that device axis names like `'data'`, `'model'` or `None` are passed into both [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) and [`jax.lax.with_sharding_constraint`](https://github.com/google/jax/blob/main/jax/_src/pjit.py#L1516) API calls. This refers to how each dimension of this data should be sharded — either across one of the device mesh dimensions, or not sharded at all.

For example:

* When you define `W1` with shape `(x.shape[-1], self.depth)` and annotate as `(None, 'model')`:

  * The first dimension (of length `x.shape[-1]`) will be replicated across all devices.
  * The second dimension (of length `self.depth`) will be sharded over the `'model'` axis of the device mesh. This means `W1` will be sharded 4-way on devices `(0, 4)`, `(1, 5)`, `(2, 6)` and `(3, 7)`, on this dimension.

* When you annotate the output `z` as `('data', None)`:

  * The first dimension — the batch dimension — will be sharded over the `'data'` axis. This means half of the batch will be processed on devices `0-3` (first four devices), and another half on devices `4-7` (the remaining four devices).
  * The second dimension — the data depth dimension — will be replicated across all devices.

## Define a model with `flax.linen.scan` lifted transformation

Having created `DotReluDot`, you can now define the `MLP` model (by subclassing [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module)) as multiple layers of `DotReluDot`.

To replicate identical layers, you can either use [`flax.linen.scan`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/transformations.html#flax.linen.scan), or a for-loop:

* `flax.linen.scan` can provide faster compilation times.
* The for-loop can be faster on runtime.

The code below shows how to apply both methods, and default with the for-loop, so that all the parameters are two-dimensional and you can visualize their sharding.

The `flax.linen.scan` code is just to show that this API works with [Flax lifted transforms](https://flax.readthedocs.io/en/latest/developer_notes/lift.html#supported-transformations).
"""



"""Now, create a `model` instance, and a sample input `x`."""

# MLP hyperparameters.
BATCH, DIM, CHANNELS, NUM_CLASS = 64, 32, 3, 10
# Create fake inputs.
x = jnp.ones((BATCH, DIM,DIM,CHANNELS))
y = jnp.ones((BATCH,))
# Initialize a PRNG key.
k = random.key(0)

# Create an Optax optimizer.
optimizer = optax.adam(learning_rate=0.001)
# Instantiate the model.
model = CNN(10)
model

def to_jax_array(np_array):
    return jnp.array(np_array)

"""## Specify sharding

Next, you need to tell `jax.jit` how to shard our data across devices.

### The input's sharding

For data parallelism, you can shard the batched _input_ `x` across the `data` axis by denoting the batch axis as `'data'`. Then, use [`jax.device_put`](https://jax.readthedocs.io/en/latest/_autosummary/jax.device_put.html) to place it onto the correct `device`s.
"""

x_sharding = mesh_sharding(PartitionSpec('data')) # dimensions: (batch, length)
y_sharding = mesh_sharding(PartitionSpec('data')) # dimensions: (batch, length)
x = jax.device_put(x, x_sharding)
y = jax.device_put(y, y_sharding)

"""### The output's sharding

You need to compile `model.init()` (that is, [`flax.linen.Module.init()`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.Module.init)), and its output as a pytree of parameters. Additionally, you may sometimes need wrap it with a [`flax.training.train_state`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.train_state.TrainState) to track other variables, such as optimizer states, and that would make the output an even more complex pytree.

To achieve this, luckily, you don't have to hardcode the output's sharding by hand. Instead, you can:

1. Evaluate `model.init` (in this case, a wrapper of it) abstractly using [`jax.eval_shape`](https://jax.readthedocs.io/en/latest/_autosummary/jax.eval_shape.html).

1. Use [`flax.linen.get_sharding`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.get_sharding) to automatically generate the `jax.sharding.NamedSharding`.
   * This step utilizes the [`flax.linen.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.with_partitioning) annotations in the earlier definition to generate the correct sharding for the parameters.
"""

def init_fn(k, x, model, optimizer):
  variables = model.init(k, x) # Initialize the model.
  state = train_state.TrainState.create( # Create a `TrainState`.
    apply_fn=model.apply,
    params=variables['params'],
    tx=optimizer)
  return state

# Create an abstract closure to wrap the function before feeding it in
# because `jax.eval_shape` only takes pytrees as arguments.
abstract_variables = jax.eval_shape(
    functools.partial(init_fn, model=model, optimizer=optimizer), k, x)

# This `state_sharding` has the same pytree structure as `state`, the output
# of the `init_fn`.
state_sharding = nn.get_sharding(abstract_variables, mesh)

"""## Compile the code

Now you can apply [`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html) to your `init_fn`, but with two extra arguments: `in_shardings` and `out_shardings`.

Run it to get the `initialized_state`, in which parameters are sharded exactly as instructed:
"""

jit_init_fn = jax.jit(init_fn, static_argnums=(2, 3),
                      in_shardings=(mesh_sharding(()), x_sharding),  # PRNG key and x
                      out_shardings=state_sharding)

state = jit_init_fn(k, x, model, optimizer)

"""## Inspect the Module output

Note that in the output of `initialized_state`, the `params` `W1` and `W2` are of type [`flax.linen.Partitioned`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/spmd.html#flax.linen.Partitioned). This is a wrapper around the actual `jax.Array` that allows Flax to record the axis names associated with it.

You can access the raw `jax.Array`s by calling `flax.linen.meta.unbox()` upon the dictionary, or call `.value` upon individual variable. You can also use `flax.linen.meta.replace_boxed()` to change the underlying `jax.Array` without modifying the sharding annotations.

You can also check the underlying [`jax.sharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html) of each parameter, which is now more internal than `NamedSharding`. Note that numbers like `initialized_state.step` are replicated across all devices.

You can use [`jax.tree_util.tree_map`](https://jax.readthedocs.io/en/latest/_autosummary/jax.tree_util.tree_map.html) to perform mass computation on a dict of boxed params, in the same way as on a dict of JAX arrays.

## Compile the train step and inference

Create a `jit`ted training step as follows:
"""

@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding, y_sharding),
                   out_shardings=state_sharding)
def train_step(state, x, y):
  # A fake loss function.
  def loss_unrolled(params):
    logits = model.apply({'params': params}, x)
    return jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=jax.nn.one_hot(y, 10)))

  grad_fn = jax.grad(loss_unrolled)
  grads = grad_fn(state.params)
  state = state.apply_gradients(grads=grads)
  return state

with mesh:
  state= train_step(state, x,y)

for e in range(10):
  print(f"Running epoch {e}")
  for (x,y) in loader:
    x = to_jax_array(x)
    y= to_jax_array(y)
    x = jnp.transpose(x,(0,2,3,1))
    x = jax.device_put(x, x_sharding)
    y = jax.device_put(y, y_sharding)

    with mesh:
      state = train_step(state,x,y)

"""Then, create a compiled inference step. Note that the output is also sharded along `(data, None)`."""

@functools.partial(jax.jit, in_shardings=(state_sharding, x_sharding),
                   out_shardings=x_sharding)
def apply_fn(state, x):
  return state.apply_fn({'params': state.params}, x)

with mesh:
  y = apply_fn(state, x)
print(type(y))
print(y.dtype)
print(y.shape)
jax.debug.visualize_array_sharding(y)
print(jnp.argmax(jax.nn.softmax(y), axis=-1))

"""### GGN vp"""

vec_sharding = mesh_sharding(PartitionSpec(None)) # dimensions: (batch, length)

@functools.partial(jax.jit, in_shardings=(vec_sharding, x_sharding),
                   out_shardings=x_sharding)
def apply_fn(params, x):
  return state.apply_fn({'params': params}, x)

params_vec, unflatten_fn = jax.flatten_util.ravel_pytree(state.params)

def model_apply_vec(params_vec, x):
  with mesh:
    y = apply_fn(unflatten_fn(params_vec), x)

params_vec, unflatten_fn = jax.flatten_util.ravel_pytree(state.params)
@functools.partial(jax.jit, in_shardings=(vec_sharding, x_sharding),
                   out_shardings=x_sharding)
def apply_fn_vec(params_vec, x):
  return state.apply_fn({'params': unflatten_fn(params_vec)}, x)

D = len(params_vec)

params_vec[0]

jax.debug.visualize_array_sharding(state.params['Dense_0']['kernel'])

jax.debug.visualize_array_sharding(params_vec)

jax.debug.visualize_array_sharding(vec)

jax.debug.visualize_array_sharding(jvp)

vec = jnp.ones((D,))
vec = jax.device_put(vec, vec_sharding)
ggn_vp = 0
# JtJ vec
for (x,y) in loader:
  x = to_jax_array(x)
  y= to_jax_array(y)
  x = jnp.transpose(x,(0,2,3,1))
  x = jax.device_put(x, x_sharding)
  y = jax.device_put(y, y_sharding)
  model_on_data = lambda p: apply_fn_vec(p, x)
  _, jvp = jax.jvp(model_on_data, (params_vec,), (vec,))
  _, vjp_fn = jax.vjp(model_on_data, params_vec)
  ggn_vp += vjp_fn(jvp)[0]

out = model_on_data(params_vec)

jax.debug.visualize_array_sharding(ggn_vp)

